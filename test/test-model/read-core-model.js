const HeaderSize=48,HeaderBufferSize=192;var fs=require("fs"),readModel=function(a){var b,c,d,e,f,g,h,i,j,k=new Uint32Array(HeaderSize),l=Object.create(null),m=function(a,c,d,e){var f=-1,g=null;try{f=fs.readSync(b,a,0,c)}catch(a){throw Error("Read Model: incorrect input \u2013 read failure at: "+d+"\n\t"+a.message)}if(f!==c)throw Error("Read Model: incorrect input length found for: "+d);if(e)try{g=JSON.parse(a.toString("utf8"))}catch(a){throw Error("Read Model: incorrect format \u2013 parse failure at: "+d+"\n\t"+a.message)}return g?g:a};try{b=fs.openSync(a,"r")}catch(a){throw Error("Read Model: file open failure\n\t"+a.message)}for(const b in k=m(k,HeaderBufferSize,"header",!1),c=m(Buffer.alloc(k[2]),k[2],"packing",!0),d=new Uint32Array(k[3]/4),d=m(d,k[3],"lexicon",!1),e=m(Buffer.alloc(k[4]),k[4],"features",!0),c.layout)if(0===c.layout[b][3]){e[b].hash=Object.create(null);for(let a=0;a<e[b].list.length;a+=1)e[b].hash[e[b].list[a]]=a}e.lexeme.hash=Object.create(null);for(let b=0;b<e.lexeme.list.length;b+=1)e.lexeme.hash[e.lexeme.list[b]]=b;g=m(Buffer.alloc(k[5]),k[5],"tcat",!0),h=m(Buffer.alloc(k[6]),k[6],"pos",!0),i=m(Buffer.alloc(k[7]),k[7],"trex",!0),f=new Uint32Array(k[8]/4),f=m(f,k[8],"xpansions",!1),j=m(Buffer.alloc(k[9]),k[9],"preserve",!0);const n=e.posClusters.list;for(let b=0;b<n.length;b+=1)n[b]=new Set(n[b].split("_").map(a=>h.hash[a]||0));return l.packing=c,l.lexicon=d,l.features=e,l.tcat=g,l.pos=h,l.trex=i,l.xpansions=f,l.preserve=j,l};module.exports=readModel;
